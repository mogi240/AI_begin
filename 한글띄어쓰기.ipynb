{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "한글띄어쓰기.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjpxmXzpzmhGaMj9M+pUy6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mogi240/AI_begin/blob/master/%ED%95%9C%EA%B8%80%EB%9D%84%EC%96%B4%EC%93%B0%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #mount안에 경로를 던져준다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mk--lDQY1SYD",
        "outputId": "315add07-0c69-4070-94ba-a93816f6d158"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from argparse import ArgumentParser\n",
        "from typing import List, Tuple\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.losses import LossFunctionWrapper\n",
        "\n",
        "parser = ArgumentParser()\n",
        "parser.add_argument(\"--train-file\", type=str, required=True)\n",
        "parser.add_argument(\"--dev-file\", type=str, required=True)\n",
        "parser.add_argument(\"--training-config\", type=str, required=True)\n",
        "parser.add_argument(\"--char-file\", type=str, required=True)\n",
        "\n",
        "\n",
        "class SpacingModel(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        hidden_size: int,\n",
        "        num_classes: int = 3,\n",
        "        conv_activation: str = \"relu\",\n",
        "        dense_activation: str = \"relu\",\n",
        "        conv_kernel_and_filter_sizes: List[Tuple[int, int]] = [\n",
        "            (2, 8),\n",
        "            (3, 8),\n",
        "            (4, 8),\n",
        "            (5, 8),\n",
        "        ],\n",
        "        dropout_rate: float = 0.3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embeddings = tf.keras.layers.Embedding(vocab_size, hidden_size)\n",
        "        self.convs = [\n",
        "            tf.keras.layers.Conv1D(\n",
        "                filter_size,\n",
        "                kernel_size,\n",
        "                padding=\"same\",\n",
        "                activation=conv_activation,\n",
        "            )\n",
        "            for kernel_size, filter_size in conv_kernel_and_filter_sizes\n",
        "        ]\n",
        "        self.pools = [\n",
        "            tf.keras.layers.MaxPooling1D(pool_size=filter_size, data_format=\"channels_first\")\n",
        "            for _, filter_size in conv_kernel_and_filter_sizes\n",
        "        ]\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate=dropout_rate)\n",
        "        self.output_dense1 = tf.keras.layers.Dense(hidden_size, activation=dense_activation)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate=dropout_rate)\n",
        "        self.output_dense2 = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "    def call(self, input_tensor):\n",
        "        \"\"\"\n",
        "        input_tensor: Tokenized Sequences, Shape: (Batch Size, Sequence Length)\n",
        "        \"\"\"\n",
        "\n",
        "        # embeddings: (Batch Size, Sequence Length, Hidden Size)\n",
        "        embeddings = self.embeddings(input_tensor)\n",
        "        # features: (Batch Size, Sequence Length, sum(#filters))\n",
        "        features = self.dropout1(\n",
        "            tf.concat([pool(conv(embeddings)) for conv, pool in zip(self.convs, self.pools)], axis=-1)\n",
        "        )\n",
        "        # projected: (Batch Size, Sequence Length, Hidden Size)\n",
        "        projected = self.dropout2(self.output_dense1(features))\n",
        "        # (Batch Size, Sequence Length, 2)\n",
        "        return self.output_dense2(projected)\n",
        "\n",
        "\n",
        "def string_to_example(\n",
        "    vocab_table: tf.lookup.StaticHashTable,\n",
        "    encoding: str = \"UTF-8\",\n",
        "    max_length: int = 256,\n",
        "    delete_prob: float = 0.5,\n",
        "    add_prob: float = 0.15,\n",
        "):\n",
        "    @tf.function\n",
        "    def _inner(tensors: tf.Tensor):\n",
        "        bytes_array = tf.strings.unicode_split(tf.strings.regex_replace(tensors, \" +\", \" \"), encoding)\n",
        "        space_positions = bytes_array == \" \"\n",
        "        sequence_length = tf.shape(space_positions)[0]\n",
        "\n",
        "        while_condition = lambda i, *_: i < sequence_length\n",
        "\n",
        "        def while_body(i, strings, labels):\n",
        "            # 다음 char가 space가 아니고, 문장 끝이 아닐 때 add_prob의 확률로 space 추가\n",
        "            # 이번 char가 space일 때\n",
        "            is_next_char_space = tf.cond(i < sequence_length - 1, lambda: bytes_array[i + 1] == \" \", lambda: False)\n",
        "\n",
        "            state = tf.cond(\n",
        "                is_next_char_space,\n",
        "                lambda: tf.cond(tf.random.uniform([]) < delete_prob, lambda: 2, lambda: 0),\n",
        "                lambda: tf.cond(bytes_array[i] != \" \" and tf.random.uniform([]) < add_prob, lambda: 1, lambda: 0),\n",
        "            )\n",
        "            # 0: 그대로 진행\n",
        "            # 1: 다음 인덱스에 space 추가\n",
        "            # 2: 다음 space 삭제\n",
        "            strings = tf.cond(\n",
        "                state != 1,\n",
        "                lambda: tf.concat([strings, [bytes_array[i]]], axis=0),\n",
        "                lambda: tf.concat([strings, [bytes_array[i], \" \"]], axis=0),\n",
        "            )\n",
        "            # label 0: 변화 x\n",
        "            # label 1: 다음 인덱스에 space 추가\n",
        "            # label 2: 현재 space 삭제\n",
        "            labels = tf.cond(\n",
        "                state == 0,\n",
        "                lambda: tf.concat([labels, [0]], axis=0),\n",
        "                lambda: tf.cond(\n",
        "                    state == 1,\n",
        "                    lambda: tf.concat([labels, [0, 2]], axis=0),\n",
        "                    lambda: tf.concat([labels, [1]], axis=0),\n",
        "                ),\n",
        "            )\n",
        "            i += tf.cond(state == 2, lambda: 2, lambda: 1)\n",
        "\n",
        "            return (i, strings, labels)\n",
        "\n",
        "        i, strings, labels = tf.while_loop(\n",
        "            while_condition,\n",
        "            while_body,\n",
        "            (\n",
        "                tf.constant(0),\n",
        "                tf.constant([], dtype=tf.string),\n",
        "                tf.constant([], dtype=tf.int32),\n",
        "            ),\n",
        "            shape_invariants=(tf.TensorShape([]), tf.TensorShape([None]), tf.TensorShape([None])),\n",
        "        )\n",
        "\n",
        "        strings = vocab_table.lookup(tf.concat([[\"<s>\"], strings, [\"</s>\"]], axis=0))\n",
        "        labels = tf.concat([[0], labels, [0]], axis=0)\n",
        "\n",
        "        strings = tf.cond(tf.shape(strings)[0] > max_length, lambda: strings[:max_length], lambda: strings)\n",
        "        labels = tf.cond(tf.shape(labels)[0] > max_length, lambda: labels[:max_length], lambda: labels)\n",
        "\n",
        "        length_to_pad = max_length - tf.shape(strings)[0]\n",
        "        strings = tf.pad(strings, [[0, length_to_pad]])\n",
        "        labels = tf.pad(labels, [[0, length_to_pad]], constant_values=-1)\n",
        "\n",
        "        return (strings, labels)\n",
        "\n",
        "    return _inner\n",
        "\n",
        "\n",
        "def sparse_categorical_crossentropy_with_ignore(y_true, y_pred, from_logits=False, axis=-1, ignore_id=-1):\n",
        "    positions = tf.where(y_true != ignore_id)\n",
        "\n",
        "    y_true = tf.gather_nd(y_true, positions)\n",
        "    y_pred = tf.gather_nd(y_pred, positions)\n",
        "\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=from_logits, axis=axis)\n",
        "\n",
        "\n",
        "def sparse_categorical_accuracy_with_ignore(y_true, y_pred, ignore_id=-1):\n",
        "    positions = tf.where(y_true != ignore_id)\n",
        "\n",
        "    y_true = tf.gather_nd(y_true, positions)\n",
        "    y_pred = tf.gather_nd(y_pred, positions)\n",
        "\n",
        "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "\n",
        "#class SparseCategoricalCrossentropyWithIgnore(tf.python.keras.losses.LossFunctionWrapper):\n",
        "class SparseCategoricalCrossentropyWithIgnore(LossFunctionWrapper):\n",
        "    def __init__(\n",
        "        self,\n",
        "        from_logits=False,\n",
        "        reduction=tf.keras.losses.Reduction.AUTO,\n",
        "        ignore_id=-1,\n",
        "        name=\"sparse_categorical_crossentropy_with_ignore\",\n",
        "    ):\n",
        "        super(SparseCategoricalCrossentropyWithIgnore, self).__init__(\n",
        "            sparse_categorical_crossentropy_with_ignore,\n",
        "            name=name,\n",
        "            reduction=reduction,\n",
        "            ignore_id=ignore_id,\n",
        "            from_logits=from_logits,\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "id": "9kz2gLmQluH8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " '''\n",
        " 모델링\n",
        " '''\n",
        "\n",
        " def main():\n",
        "    print('main start....')\n",
        "    \n",
        "    config =  {\n",
        "    \"train_batch_size\": 64,\n",
        "    \"val_batch_size\": 1024,\n",
        "    \"epochs\": 5,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"vocab_size\": 5000,\n",
        "    \"hidden_size\": 48,\n",
        "    \"conv_activation\": \"relu\",\n",
        "    \"dense_activation\": \"relu\",\n",
        "    \"conv_kernel_and_filter_sizes\": [[2, 8], [3, 8], [4, 8], [5, 16], [6, 16], [7, 16], [8, 16], [9, 16], [10, 16]],\n",
        "    \"dropout_rate\": 0.1\n",
        "    }\n",
        "\n",
        "    with open('/content/drive/MyDrive/input/hangeul/chars-4996') as f:\n",
        "    #with open(args.char_file) as f:\n",
        "        content = f.read()\n",
        "        keys = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"] + list(content)\n",
        "        values = list(range(len(keys)))\n",
        "\n",
        "    vocab_initializer = tf.lookup.KeyValueTensorInitializer(keys, values, key_dtype=tf.string, value_dtype=tf.int32)\n",
        "    vocab_table = tf.lookup.StaticHashTable(vocab_initializer, default_value=3)\n",
        "\n",
        "    train_dataset = (\n",
        "        tf.data.TextLineDataset(tf.constant('/content/drive/MyDrive/input/hangeul/namuwikitext_20200302.train.zip'))\n",
        "        .shuffle(10000)\n",
        "        .map(\n",
        "            string_to_example(vocab_table),\n",
        "            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
        "        )\n",
        "        .batch(config[\"train_batch_size\"])\n",
        "    )\n",
        "    dev_dataset = (\n",
        "        tf.data.TextLineDataset(tf.constant('/content/drive/MyDrive/input/hangeul/namuwikitext_20200302.dev.zip'))\n",
        "        .shuffle(10000)\n",
        "        .map(\n",
        "            string_to_example(vocab_table),\n",
        "            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
        "        )\n",
        "        .batch(config[\"val_batch_size\"])\n",
        "        .take(4)\n",
        "    )\n",
        "\n",
        "    model = SpacingModel(\n",
        "        config[\"vocab_size\"],\n",
        "        config[\"hidden_size\"],\n",
        "        conv_activation=config[\"conv_activation\"],\n",
        "        dense_activation=config[\"dense_activation\"],\n",
        "        conv_kernel_and_filter_sizes=config[\"conv_kernel_and_filter_sizes\"],\n",
        "        dropout_rate=config[\"dropout_rate\"],\n",
        "    )\n",
        "    model.compile(\n",
        "        optimizer=tf.optimizers.Adam(learning_rate=config[\"learning_rate\"]),\n",
        "        loss=SparseCategoricalCrossentropyWithIgnore(from_logits=True, ignore_id=-1),\n",
        "        metrics=[sparse_categorical_accuracy_with_ignore],\n",
        "    )\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        epochs=config[\"epochs\"],\n",
        "        validation_data=dev_dataset,\n",
        "        steps_per_epoch=400,\n",
        "        callbacks=[\n",
        "            tf.keras.callbacks.ModelCheckpoint(filepath=\"./models/checkpoint-{epoch}.ckpt\"),\n",
        "            tf.keras.callbacks.TensorBoard(log_dir=\"./logs\"),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # tf.saved_model.save(\n",
        "    #     model,\n",
        "    #     '/content/drive/MyDrive/input/',\n",
        "    #     serve.get_concrete_function(tf.TensorSpec(shape=[None, None], dtype=tf.int32, name=\"input_tensor\")),\n",
        "    # )\n",
        "    tf.saved_model.save(\n",
        "        model\n",
        "        ,'/content/drive/MyDrive/input/hangeul/my_custom_model_1'\n",
        "        , signatures=None, options=None\n",
        "    )\n",
        "    model.save_weights('/content/drive/MyDrive/input/hangeul/weight/my_custom_model_weight')\n",
        "\n",
        "    model = SpacingModel(\n",
        "    config[\"vocab_size\"],\n",
        "    config[\"hidden_size\"],\n",
        "    conv_activation=config[\"conv_activation\"],\n",
        "    dense_activation=config[\"dense_activation\"],\n",
        "    conv_kernel_and_filter_sizes=config[\"conv_kernel_and_filter_sizes\"],\n",
        "    dropout_rate=config[\"dropout_rate\"],\n",
        "    )\n",
        "    model.load_weights('/content/drive/MyDrive/input/hangeul/weight/my_custom_model_weight')\n",
        "\n",
        "\n",
        "    @tf.function()\n",
        "    def serve(input_tensor):\n",
        "        return model(input_tensor)\n",
        "\n",
        "\n",
        "    tf.saved_model.save(\n",
        "        model,\n",
        "        '/content/drive/MyDrive/input/hangeul/my_custom_model_2',\n",
        "        serve.get_concrete_function(tf.TensorSpec(shape=[None, None], dtype=tf.int32, name=\"input_tensor\")),\n",
        "    )\n",
        "\n",
        "    print('main end....')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMuHqF9eCVJf",
        "outputId": "2a89176c-b8d5-43a3-eabd-ad0f87097c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main start....\n",
            "Epoch 1/30\n",
            "400/400 [==============================] - ETA: 0s - loss: 0.0420 - sparse_categorical_accuracy_with_ignore: 0.9905INFO:tensorflow:Assets written to: ./models/checkpoint-1.ckpt/assets\n",
            "400/400 [==============================] - 429s 1s/step - loss: 0.0420 - sparse_categorical_accuracy_with_ignore: 0.9905 - val_loss: 0.0217 - val_sparse_categorical_accuracy_with_ignore: 0.9966 - lr: 0.0100\n",
            "Epoch 2/30\n",
            "400/400 [==============================] - ETA: 0s - loss: 0.0213 - sparse_categorical_accuracy_with_ignore: 0.9966INFO:tensorflow:Assets written to: ./models/checkpoint-2.ckpt/assets\n",
            "400/400 [==============================] - 406s 1s/step - loss: 0.0213 - sparse_categorical_accuracy_with_ignore: 0.9966 - val_loss: 0.0206 - val_sparse_categorical_accuracy_with_ignore: 0.9967 - lr: 0.0100\n",
            "Epoch 3/30\n",
            "400/400 [==============================] - ETA: 0s - loss: 0.0212 - sparse_categorical_accuracy_with_ignore: 0.9967INFO:tensorflow:Assets written to: ./models/checkpoint-3.ckpt/assets\n",
            "400/400 [==============================] - 407s 1s/step - loss: 0.0212 - sparse_categorical_accuracy_with_ignore: 0.9967 - val_loss: 0.0207 - val_sparse_categorical_accuracy_with_ignore: 0.9967 - lr: 0.0100\n",
            "Epoch 4/30\n",
            "400/400 [==============================] - ETA: 0s - loss: 0.0209 - sparse_categorical_accuracy_with_ignore: 0.9967INFO:tensorflow:Assets written to: ./models/checkpoint-4.ckpt/assets\n",
            "400/400 [==============================] - 413s 1s/step - loss: 0.0209 - sparse_categorical_accuracy_with_ignore: 0.9967 - val_loss: 0.0202 - val_sparse_categorical_accuracy_with_ignore: 0.9968 - lr: 0.0100\n",
            "Epoch 5/30\n",
            "400/400 [==============================] - ETA: 0s - loss: 0.0211 - sparse_categorical_accuracy_with_ignore: 0.9967INFO:tensorflow:Assets written to: ./models/checkpoint-5.ckpt/assets\n",
            "400/400 [==============================] - 405s 1s/step - loss: 0.0211 - sparse_categorical_accuracy_with_ignore: 0.9967 - val_loss: 0.0208 - val_sparse_categorical_accuracy_with_ignore: 0.9967 - lr: 0.0100\n",
            "Epoch 6/30\n",
            "400/400 [==============================] - ETA: 0s - loss: 0.0214 - sparse_categorical_accuracy_with_ignore: 0.9966INFO:tensorflow:Assets written to: ./models/checkpoint-6.ckpt/assets\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "400/400 [==============================] - 436s 1s/step - loss: 0.0214 - sparse_categorical_accuracy_with_ignore: 0.9966 - val_loss: 0.0204 - val_sparse_categorical_accuracy_with_ignore: 0.9967 - lr: 0.0100\n",
            "Epoch 7/30\n",
            "400/400 [==============================] - ETA: 0s - loss: 0.0210 - sparse_categorical_accuracy_with_ignore: 0.9967INFO:tensorflow:Assets written to: ./models/checkpoint-7.ckpt/assets\n",
            "400/400 [==============================] - 434s 1s/step - loss: 0.0210 - sparse_categorical_accuracy_with_ignore: 0.9967 - val_loss: 0.0207 - val_sparse_categorical_accuracy_with_ignore: 0.9967 - lr: 1.0000e-03\n",
            "Epoch 8/30\n",
            "400/400 [==============================] - ETA: 0s - loss: 0.0206 - sparse_categorical_accuracy_with_ignore: 0.9967INFO:tensorflow:Assets written to: ./models/checkpoint-8.ckpt/assets\n",
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
            "400/400 [==============================] - 412s 1s/step - loss: 0.0206 - sparse_categorical_accuracy_with_ignore: 0.9967 - val_loss: 0.0209 - val_sparse_categorical_accuracy_with_ignore: 0.9967 - lr: 1.0000e-03\n",
            "Epoch 9/30\n",
            "400/400 [==============================] - ETA: 0s - loss: 0.0208 - sparse_categorical_accuracy_with_ignore: 0.9967INFO:tensorflow:Assets written to: ./models/checkpoint-9.ckpt/assets\n",
            "400/400 [==============================] - 436s 1s/step - loss: 0.0208 - sparse_categorical_accuracy_with_ignore: 0.9967 - val_loss: 0.0207 - val_sparse_categorical_accuracy_with_ignore: 0.9967 - lr: 1.0000e-04\n",
            "Epoch 10/30\n",
            "400/400 [==============================] - ETA: 0s - loss: 0.0206 - sparse_categorical_accuracy_with_ignore: 0.9967INFO:tensorflow:Assets written to: ./models/checkpoint-10.ckpt/assets\n",
            "\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
            "400/400 [==============================] - 412s 1s/step - loss: 0.0206 - sparse_categorical_accuracy_with_ignore: 0.9967 - val_loss: 0.0202 - val_sparse_categorical_accuracy_with_ignore: 0.9968 - lr: 1.0000e-04\n",
            "Epoch 11/30\n",
            "400/400 [==============================] - ETA: 0s - loss: 0.0205 - sparse_categorical_accuracy_with_ignore: 0.9967INFO:tensorflow:Assets written to: ./models/checkpoint-11.ckpt/assets\n",
            "400/400 [==============================] - 435s 1s/step - loss: 0.0205 - sparse_categorical_accuracy_with_ignore: 0.9967 - val_loss: 0.0205 - val_sparse_categorical_accuracy_with_ignore: 0.9967 - lr: 1.0000e-05\n",
            "Epoch 12/30\n",
            "400/400 [==============================] - ETA: 0s - loss: 0.0210 - sparse_categorical_accuracy_with_ignore: 0.9967INFO:tensorflow:Assets written to: ./models/checkpoint-12.ckpt/assets\n",
            "\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
            "400/400 [==============================] - 432s 1s/step - loss: 0.0210 - sparse_categorical_accuracy_with_ignore: 0.9967 - val_loss: 0.0208 - val_sparse_categorical_accuracy_with_ignore: 0.9967 - lr: 1.0000e-05\n",
            "Epoch 13/30\n",
            "400/400 [==============================] - ETA: 0s - loss: 0.0207 - sparse_categorical_accuracy_with_ignore: 0.9967INFO:tensorflow:Assets written to: ./models/checkpoint-13.ckpt/assets\n",
            "400/400 [==============================] - 438s 1s/step - loss: 0.0207 - sparse_categorical_accuracy_with_ignore: 0.9967 - val_loss: 0.0209 - val_sparse_categorical_accuracy_with_ignore: 0.9966 - lr: 1.0000e-06\n",
            "Epoch 14/30\n",
            "400/400 [==============================] - ETA: 0s - loss: 0.0209 - sparse_categorical_accuracy_with_ignore: 0.9967INFO:tensorflow:Assets written to: ./models/checkpoint-14.ckpt/assets\n",
            "400/400 [==============================] - 413s 1s/step - loss: 0.0209 - sparse_categorical_accuracy_with_ignore: 0.9967 - val_loss: 0.0195 - val_sparse_categorical_accuracy_with_ignore: 0.9969 - lr: 1.0000e-06\n",
            "Epoch 15/30\n",
            "400/400 [==============================] - ETA: 0s - loss: 0.0210 - sparse_categorical_accuracy_with_ignore: 0.9966INFO:tensorflow:Assets written to: ./models/checkpoint-15.ckpt/assets\n",
            "400/400 [==============================] - 438s 1s/step - loss: 0.0210 - sparse_categorical_accuracy_with_ignore: 0.9966 - val_loss: 0.0204 - val_sparse_categorical_accuracy_with_ignore: 0.9967 - lr: 1.0000e-06\n",
            "Epoch 16/30\n",
            "266/400 [==================>...........] - ETA: 1:56 - loss: 0.0207 - sparse_categorical_accuracy_with_ignore: 0.9967"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "모델평가\n",
        "'''\n",
        "\n",
        "import json\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# from train import (\n",
        "#     SpacingModel,\n",
        "#     string_to_example,\n",
        "#     sparse_categorical_accuracy_with_ignore,\n",
        "#     SparseCategoricalCrossentropyWithIgnore,\n",
        "# )\n",
        "\n",
        "# parser = ArgumentParser()\n",
        "# parser.add_argument(\"--char-file\", type=str, required=True)\n",
        "# parser.add_argument(\"--model-file\", type=str, required=True)\n",
        "# parser.add_argument(\"--training-config\", type=str, required=True)\n",
        "# parser.add_argument(\"--test-file\", type=str, required=True)\n",
        "# parser.add_argument(\"--add-prob\", type=float, required=True)\n",
        "# parser.add_argument(\"--delete-prob\", type=float, required=True)\n",
        "\n",
        "\n",
        "def main():\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    config =  {\n",
        "    \"train_batch_size\": 64,\n",
        "    \"val_batch_size\": 1024,\n",
        "    \"epochs\": 30,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"vocab_size\": 5000,\n",
        "    \"hidden_size\": 48,\n",
        "    \"conv_activation\": \"relu\",\n",
        "    \"dense_activation\": \"relu\",\n",
        "    \"conv_kernel_and_filter_sizes\": [[2, 8], [3, 8], [4, 8], [5, 16], [6, 16], [7, 16], [8, 16], [9, 16], [10, 16]],\n",
        "    \"dropout_rate\": 0.1\n",
        "    }\n",
        "\n",
        "    with open('/content/drive/MyDrive/input/hangeul/chars-4996') as f:\n",
        "        content = f.read()\n",
        "        keys = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"] + list(content)\n",
        "        values = list(range(len(keys)))\n",
        "\n",
        "    vocab_initializer = tf.lookup.KeyValueTensorInitializer(keys, values, key_dtype=tf.string, value_dtype=tf.int32)\n",
        "    vocab_table = tf.lookup.StaticHashTable(vocab_initializer, default_value=3)\n",
        "\n",
        "    test_dataset = (\n",
        "        tf.data.TextLineDataset('/content/drive/MyDrive/input/hangeul/namuwikitext_20200302.test.zip')).shuffle(10000).map(\n",
        "            string_to_example(vocab_table),\n",
        "            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
        "        ).batch(config[\"val_batch_size\"])\n",
        "\n",
        "    model = SpacingModel(\n",
        "        config[\"vocab_size\"],\n",
        "        config[\"hidden_size\"],\n",
        "        conv_activation=config[\"conv_activation\"],\n",
        "        dense_activation=config[\"dense_activation\"],\n",
        "        conv_kernel_and_filter_sizes=config[\"conv_kernel_and_filter_sizes\"],\n",
        "        dropout_rate=config[\"dropout_rate\"],\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.optimizers.Adam(learning_rate=config[\"learning_rate\"]),\n",
        "        loss=SparseCategoricalCrossentropyWithIgnore(from_logits=True, ignore_id=-1),\n",
        "        metrics=[sparse_categorical_accuracy_with_ignore],\n",
        "    )\n",
        "\n",
        "    model.load_weights('/content/drive/MyDrive/input/hangeul/weight/my_custom_model_weight')\n",
        "    model(tf.keras.Input([None], dtype=tf.int32))\n",
        "    model.summary()\n",
        "    model.evaluate(test_dataset)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "o6df7AMult-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "시뮬레이션\n",
        "'''\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def main():\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "   \n",
        "\n",
        "    config =  {\n",
        "    \"train_batch_size\": 64,\n",
        "    \"val_batch_size\": 1024,\n",
        "    \"epochs\": 30,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"vocab_size\": 5000,\n",
        "    \"hidden_size\": 48,\n",
        "    \"conv_activation\": \"relu\",\n",
        "    \"dense_activation\": \"relu\",\n",
        "    \"conv_kernel_and_filter_sizes\": [[2, 8], [3, 8], [4, 8], [5, 16], [6, 16], [7, 16], [8, 16], [9, 16], [10, 16]],\n",
        "    \"dropout_rate\": 0.1\n",
        "    }\n",
        "\n",
        "    with open('/content/drive/MyDrive/input/hangeul/chars-4996') as f:\n",
        "    #with open(args.char_file) as f:\n",
        "        content = f.read()\n",
        "        keys = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"] + list(content)\n",
        "        values = list(range(len(keys)))\n",
        "\n",
        "    vocab_initializer = tf.lookup.KeyValueTensorInitializer(keys, values, key_dtype=tf.string, value_dtype=tf.int32)\n",
        "    vocab_table = tf.lookup.StaticHashTable(vocab_initializer, default_value=3)\n",
        "\n",
        "    model = SpacingModel(\n",
        "        config[\"vocab_size\"],\n",
        "        config[\"hidden_size\"],\n",
        "        conv_activation=config[\"conv_activation\"],\n",
        "        dense_activation=config[\"dense_activation\"],\n",
        "        conv_kernel_and_filter_sizes=config[\"conv_kernel_and_filter_sizes\"],\n",
        "        dropout_rate=config[\"dropout_rate\"],\n",
        "    )\n",
        "\n",
        "    model.load_weights('/content/drive/MyDrive/input/hangeul/orginal/variables/')\n",
        "    #model.load_weights('/content/drive/MyDrive/input/hangeul/my_custom_model_2')\n",
        "    model(tf.keras.Input([None], dtype=tf.int32))\n",
        "    model.summary()\n",
        "\n",
        "    #tf.keras.models.load_model('/content/drive/MyDrive/input/hangeul/my_custom_model_2')\n",
        "\n",
        "\n",
        "    inference = get_inference_fn(model, vocab_table)\n",
        "\n",
        "    while True:\n",
        " #       %%time\n",
        "        input_str = input(\"Str: \")\n",
        "        start = time.process_time()\n",
        "        input_str = tf.constant(input_str)\n",
        "        result = inference(input_str).numpy()\n",
        "        end = time.process_time()\n",
        "        print(\"Time elapsed: \", end - start)  # seconds\n",
        "        print(b\"\".join(result).decode(\"utf8\"))\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "def get_inference_fn(model, vocab_table):\n",
        "    @tf.function\n",
        "    def inference(tensors):\n",
        "        byte_array = tf.concat(\n",
        "            [[\"<s>\"], tf.strings.unicode_split(tf.strings.regex_replace(tensors, \" +\", \" \"), \"UTF-8\"), [\"</s>\"]], axis=0\n",
        "        )\n",
        "        strings = vocab_table.lookup(byte_array)[tf.newaxis, :]\n",
        "\n",
        "        model_output = tf.argmax(model(strings), axis=-1)[0]\n",
        "        return convert_output_to_string(byte_array, model_output)\n",
        "\n",
        "    return inference\n",
        "\n",
        "\n",
        "def convert_output_to_string(byte_array, model_output):\n",
        "    sequence_length = tf.size(model_output)\n",
        "    while_condition = lambda i, *_: i < sequence_length\n",
        "\n",
        "    def while_body(i, o):\n",
        "        o = tf.cond(\n",
        "            model_output[i] == 1,\n",
        "            lambda: tf.concat([o, [byte_array[i], \" \"]], axis=0),\n",
        "            lambda: tf.cond(\n",
        "                (model_output[i] == 2) and (byte_array[i] == \" \"),\n",
        "                lambda: o,\n",
        "                lambda: tf.concat([o, [byte_array[i]]], axis=0),\n",
        "            ),\n",
        "        )\n",
        "        return i + 1, o\n",
        "\n",
        "    _, strings_result = tf.while_loop(\n",
        "        while_condition,\n",
        "        while_body,\n",
        "        (tf.constant(0), tf.constant([], dtype=tf.string)),\n",
        "        shape_invariants=(tf.TensorShape([]), tf.TensorShape([None])),\n",
        "    )\n",
        "    return strings_result\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "HbCUmgySPCql"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}